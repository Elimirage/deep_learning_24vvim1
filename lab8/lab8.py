#ссылка на Colab https://colab.research.google.com/drive/1hlNoBfXJQdQ_GWRn0PkULVXAMCzUQs0E?usp=sharing
# -*- coding: utf-8 -*-
"""Копия LLM_local.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hlNoBfXJQdQ_GWRn0PkULVXAMCzUQs0E
"""

# Любая LLM работает с токенами, поэтому нам нужна не только модель, но и токенизатор
from transformers import AutoModelForCausalLM, AutoTokenizer

# Зададим название модели из репозитория huggingface
# список доступных моделей можно посмотреть по ссылке https://huggingface.co/models
# будем использовать модель Qwen или YandexGPT

model_name = "Qwen/Qwen2.5-7B-Instruct-1M"
#model_name = "yandex/YandexGPT-5-Lite-8B-instruct"

# В названии модели обычно указываются - версия (2,5) количество параметров (7 млрд.)
# этап обучения или предназначение (Base, Instruct, Code, Thinking и т.п.)
# размер контекстного окна (1 млн.)
# также могут указываться параметры квантования (FP8, GGUF, ) и другие характеристики.


# Загружаем предобученную можель
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# Загружаем предобученный токенизатор
tokenizer = AutoTokenizer.from_pretrained(model_name)

file = 'ENG_article.txt'
with open(file, 'r', encoding='utf-8') as file:
    data = file.read().replace('\n', ' ')

print(f"Длина текста: {len(data)} символов")
print(f"Количество слов: {len(data.split())}")

messages = [
    {"role": "system", "content": "Проанализируй текст и найди точные ответы на вопросы. Отвечай кратко и только по фактам."},
    {"role": "user", "content":
     f"""
     Текст: {data[:15000]}

     Вопросы:
     1. В каком году была обозначена проблема взрывающихся градиентов?
     2. Кто в 1891 году разработал метод уничтожающей производной?
     3. Кто предложил цепное правило дифференцирования и в каком году?
     """}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

print("Промпт:")
print(text[:500])

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=300,
    temperature=0.7
)

generated_ids_ = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids_, skip_special_tokens=True)[0]

print(response)
