{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
  # –£–±—Ä–∞–ª –ø—Ä–æ–±–ª–µ–º–Ω—ã–π widgets –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header-info"
   },
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ ENG_article.txt —Å –ø–æ–º–æ—â—å—é LLM\n",
    "\n",
    "## –ó–∞–¥–∞–Ω–∏–µ: –ù–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ —Ç–µ–∫—Å—Ç–µ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-libs"
   },
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "# !pip install transformers accelerate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "import-libs"
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load-model",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": []
    },
    "outputId": "e6d0a81d-3e28-48e4-82e5-c0d6af2516a3"
   },
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(\"–ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞:\", model.device)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "load-file",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a8b69434-70d0-4a32-991e-a9f50d7d76e2"
   },
   "source": [
    "# –ß–∏—Ç–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ñ–∞–π–ª\n",
    "file_path = 'ENG_article.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    print(f\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!\")\n",
    "    print(f\"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(data)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {len(data.split())}\")\n",
    "    print(\"\\n–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(data[:500])\n",
    "    print(\"-\" * 50)\n",
    "except FileNotFoundError:\n",
    "    print(f\"–û—à–∏–±–∫–∞: –§–∞–π–ª '{file_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    data = \"\"\"\n",
    "    The problem of exploding gradients was first identified in 1991 by Sepp Hochreiter. \n",
    "    In 1891, the destructive derivative method was developed by Gottfried Wilhelm Leibniz. \n",
    "    The chain rule of differentiation was proposed by Isaac Newton in 1667.\n",
    "    \"\"\""
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "create-prompt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97a6c11a-317e-48a3-cfea-470c47aad04b"
   },
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful assistant that extracts precise information from texts. Answer only based on the provided text.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"\"\"\n",
    "        TEXT:\n",
    "        {data[:10000]}\n",
    "        \n",
    "        QUESTIONS:\n",
    "        1. In what year was the exploding gradients problem identified?\n",
    "        2. Who developed the destructive derivative method in 1891?\n",
    "        3. Who proposed the chain rule of differentiation and in what year?\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        - Answer each question separately\n",
    "        - Be precise and concise\n",
    "        - If information is not found, say 'Not found in text'\n",
    "        - Format: \n",
    "          1. [Answer for question 1]\n",
    "          2. [Answer for question 2]\n",
    "          3. [Answer for question 3]\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"–ü—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω!\")\n",
    "print(\"\\n–ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–æ–º–ø—Ç–∞:\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt[:1000])\n",
    "print(\"=\" * 60)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "generate-response",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0fd3b040-a019-4de6-f3a2-b56d9d08e2e4"
   },
   "source": [
    "# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ (–ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ user –ø—Ä–æ–º–ø—Ç–∞)\n",
    "response_parts = full_response.split(\"<|im_start|>assistant\")\n",
    "if len(response_parts) > 1:\n",
    "    response = response_parts[-1].strip()\n",
    "else:\n",
    "    response = full_response\n",
    "\n",
    "print(\"–ì–æ—Ç–æ–≤–æ!\")"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "display-results",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8a60f5e2-7a0b-4581-8f83-b8eeb20e450b"
   },
   "source": [
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "print(\"=\" * 60)\n",
    "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê ENG_article.txt\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n–í–û–ü–†–û–°–´:\\n\")\n",
    "print(\"1. In what year was the exploding gradients problem identified?\")\n",
    "print(\"2. Who developed the destructive derivative method in 1891?\")\n",
    "print(\"3. Who proposed the chain rule of differentiation and in what year?\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–û–¢–í–ï–¢–´ LLM:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–≤–µ—Ç –≤ —Ñ–∞–π–ª—ã\n",
    "with open('prompt_ENG.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(prompt)\n",
    "    \n",
    "with open('response_ENG.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(response)\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
    "print(\"  - prompt_ENG.txt (–ø—Ä–æ–º–ø—Ç)\")\n",
    "print(\"  - response_ENG.txt (–æ—Ç–≤–µ—Ç LLM)\")\n",
    "print(\"=\" * 60)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìä –ò—Ç–æ–≥–∏\n",
    "\n",
    "1. **–ú–æ–¥–µ–ª—å:** Qwen/Qwen2.5-7B-Instruct-1M\n",
    "2. **–ó–∞–¥–∞—á–∞:** –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "3. **–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:**\n",
    "   - `prompt_ENG.txt` - –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
    "   - `response_ENG.txt` - –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏"
   ]
  }
 ]
}
